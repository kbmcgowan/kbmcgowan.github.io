[
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "cv",
    "section": "",
    "text": "Download PDF\n    Curriculum Vitae \n\n    \n\n\n\n \n         \n             Education\n    \n    2006 - 2011 Ph.D. Linguistics \n     University of Michigan \n    Dissertation title: The Role of Socioindexical Expectation in Speech Perception\n    Steven P. Abney & Patrice Speeter Beddor, co-chairs\n    Julie Boland, Robin Queen, & Benjamin Munson, readers\n    1996  B.A. Linguistics\n    University of Michigan\n\n    2007 Summer Institute of the Linguistic Society of America\n    Stanford University\n    2006 JHU Summer School on Human Language Technology\n    Johns Hopkins University\n    \n        \n\n\n                    Employment\n    \n    2021 -   Associate Professor, Department of Linguistics, University of Kentucky\n    2015 - 2021 Assistant Professor, Program in Linguistics, Department of English, University of Kentucky\n    2013 - 2015 Postdoctoral Scholar, Department of Linguistics, Stanford UniversityMeghan Sumner (PI)\n    2011 - 2013 Lecturer, Department of Linguistics, Rice University\n    \n\n\n\n                    Professional Experience\n    \n    2019 Invited Faculty, “Speech Perception”, Linguistic Society of America Summer Institute, University of California, Davis\n    2017 Invited Faculty, “Doing phonetics research” and “Eye tracking for linguistics research” Linguistic Society of America Summer Institute, University of Kentucky\n    2013 Invited Faculty, “Introduction to Praat” Linguistic Society of America Summer Institute, University of Michigan\n    2007 - 2010 Journal Assistant & Student Board Member, Language Learning Journal, Blackwell Publishing\n    2009 - 2010 Research Assistant, Professor Pam Beddor, Michigan Phonetics & Phonology Laboratory\n    2007 - 2008 Research Assistant, Professors Pam Beddor & A. Coetzee, Michigan Phonetics & Phonology Laboratory\n    1997 - 2007 Programmer &, System administrator, Manager of Web and Database production services, University of Michigan\n    \n\n\n\n    Publications & Presentations\n    Publications ( bibtex )\n\n    \n    \n\n   (in prep)Laycock, K. \\& McGowan, K.B. Removing the disguise: the matched guise technique and listener awareness. Journal of Sociolinguistics.\n   (in press)McGowan, K.B. Speech Perception.  In Sociophonetics: Implications for Phonetics and Phonology, ed. by Lauren Hall-Lew and Jennifer Nycz.  Oxford University Press\n    \n    (2022) Barrett, R., Cramer, J., & McGowan, K. B. English with an accent: Language, ideology, and discrimination in the United States. Taylor & Francis.\n\n    (2021)Babel, A.M., McGowan, K.B., & Enríquez Duque, P. Niveles de percepción de las vocales en contacto: el caso de una variedad de español andino en Bolivia. Niveles de percepción de las vocales en contacto: el caso de una variedad de español andino en Bolivia, 119-136.\n    \n    (2020) Baese-Berk, M.M., McLaughlin, D., and McGowan, K.B. Perception of non-native speech. Language and Linguistics Compass\n\n    (2020) Medeiros, D.J., Mains, P., and McGowan, K.B. Ceiling Effects on Weight in Heavy NP Shift.Linguistic Inquiry\n\n    (2020) McGowan, K.B. & Babel, A.M. Perceiving isn’t believing: Divergence in levels of sociolinguistic awareness Language in Society 49(1)\n\n    (2019) McGowan, K.B., Johnson, M.T., Combs, A., and Soleymanpoor, M. Acoustic, non-invasive measurement of velopharyngeal aperture using a high frequency tone, Proceedings of the 19th International Congress of Phonetic Sciences, Sydney, Australia\n\n    2018 Beddor, P.S., Coetzee, A., Styler, W., McGowan, K.B., & Boland, J.E. The time course of individuals’ perception and production of coarticulatory information, and its implications for sound change Language 94(4) 931–968 \n\n    2016 McGowan, K.B. “Sounding Chinese and Listening Chinese: Awareness and Knowledge in the Laboratory”  In Awareness and Control in Sociolinguistics Research, ed. by A.M. Babel. Cambridge University Press.\n\n    2015\n    McGowan, K.B. “Social Expectation Improves Speech Perception in Noise” Language and Speech. doi:10.1177/0023830914565191\n\n    2014\n    Sumner, M., Kim, S. K., King, E., and McGowan, K.B. “The socially-weighted encoding of spoken words: A dual-route approach to speech perception” Frontiers in Psychology.  doi:10.3389/fpsyg.2013.01015\n\n    2013\n    P.S. Beddor, K.B. McGowan, J.E. Boland, A.W. Coetzee and A. Brasher “The Perceptual Time Course of Coarticulation” — Journal of the Acoustical Society of America, 133 (2013) : 2350-2366.  doi:10.1121/1.3249491\n\n    2012\n    McGowan, K.B. “Gradient Lexical Reflexes of the Syllable Contact Law” Proceedings of the Annual Meeting of the Chicago Linguistics Society 45, Volume I, R. Bochnak, N. Nicola, P. Klecha, J. Urban, A. Lemieux and C.  Weaver (Eds.), p. 445-454. Chicago Linguistics Society: Chicago.   \n    \n\n\n\n\n\n    Invited Talks & Colloquia\n    \n    2019 University of Tennessee, Knoxville — Social expectation and speech perception: mismatch and alignment\n    2016 University of Michigan — “Subcategorical mismatches can be mismatches of phonetic, phonological, lexical, and social context”\n    2016 North Carolina State University — “Socially informed speech perception: context, congruency, and complications”\n    2016 University at Buffalo, The State University of New York — “Social knowledge and speech perception: mismatch and alignment”\n    2014 UC Berkeley, Phorum — “Perceiving isn't believing: listeners' expectation and awareness of phonetically-cued social information” with A.M. Babel\n    2012 Rice University — “Claims about processing require online tasks: disjunction of AXB and interview results in a Bolivian crossroads”\n    2012  University of Texas, Austin — “Social expectation and theories of speech perception” —  Invited colloquium\n    2011  Rice University — “The role of socioindexical expectation in speech perception.”\n    2010 University of Michigan — “The perceptual time course of coarticulation” with P.S. Beddor\n    2009 University of Michigan  — “Aerodynamic Modeling of Coarticulation Improves Concatenative Synthesis”\n    \n\n\n\n    Presentations\n    \n    2015 McGowan, K.B. and Sumner, M. A phonetic explanation of pronunciation variant effects.  89th Annual Meeting of the Linguistic Society of America\n    2015 Sumner, M., McGowan, K.B., D'Onofrio, A., and Pratt, T.  The contribution of form and meaning to the processing of careful and casual speech.  89th Annual Meeting of the Linguistic Society of America\n    2015 Mains, P., McGowan, K.B., and Medeiros, D.  Gradient acceptability by length in heavy NP shift. 89th Annual Meeting of the Linguistic Society of America\n    2015 Moores, N.P., McGowan, K.B., Sumner, M. and Frank, MC. Children use phonetically-cued talker information to infer speaker meaning. Poster, 89th Annual Meeting of the Linguistic Society of America\n    2014 Moores, N.P., McGowan, K.B., Sumner, M., and Frank, M.C. Children use phonetically-cued talker information to disambiguate similar objects. The 55th Annual Meeting of the Psychonomics Society, Long Beach, CA\n    2014 McGowan, K.B. Sumner, M., D'Onofrio, A., and Pratt, T.  The contribution of form and meaning to the processing of careful and casual speech.  27th Annual CUNY Conference on Human Sentence Processing.  Columbus, OH\n    2014 Sumner, M., Calder, J., D'Onofrio, A., McGowan, K.B., and Pratt, T.  Top-down vs. bottom-up processing depends on the acoustic composition of an utterance. 88th Annual Meeting of the Linguistic Society of America\n    2013 Sumner, M.; D'Onofrio, A.; McGowan, K.; Pratt, T.; and Calder, J.  Differences in the recognition of careful and casual speech. Poster, 166th Meeting of the Acoustical Society of America\n    2013  McGowan, K.B. and Babel, A.M. Perception of Spanish in Contact: Bolivian listeners' expectation and awareness of socioindexical variation. Workshop on Sound Change Actuation, University of Chicago\n    2013 McGowan, K.B. Sounding Chinese and Listening Chinese: Imitation, Perception, and Awareness of Non-Native Phonology. Part of panel, Awareness \\& control in sociolinguistic research.  87th Annual Meeting of the Linguistic Society of America\n    2012 Babel, A.M. and McGowan, K.B.  Social Categorization and Levels of Awareness: A Cross-Disciplinary Study.  Part of panel, Voices in Movement: Phonetic Border Crossings.  American Anthropological Association annual meeting.\n    2012 McGowan, K.B. The influence of socioindexical expectations on speech perception in noise.  86th Annual Meeting of the Linguistic Society of America\n    2011 McGowan, K.B. The influence of socioindexical expectations on speech perception in noise.  Poster, 162nd Meeting of the Acoustical Society of America\n    2011 McGowan, K.B. and Medeiros, D. J.  Tongues don't twist  —mental representations do.  85th Annual Meeting of the Linguistic Society of America\n    2011 McGowan, K.B. Are you experienced? Socio-indexical knowledge and naive listeners.  Poster, 85th Annual Meeting of the Linguistic Society of America\n    2010 McGowan, K.B. Examining Listeners' Use of Sociolinguistic Information During Early Phonetic Judgments: Evidence from eye-tracking.  New Ways of Analyzing Variation (NWAV 39)\n    2010 McGowan, K.B. Listener Expectations and the Processing of Foreign-Accented Speech. Mid-Continental Workshop on Phonology (MCWOP 16) \n    2010  Beddor, P.S., McGowan, K.B., Boland, J., and Coetzee, A. The Perceptual Time Course of Coarticulation.  Poster, Laboratory Phonology (LabPhon 12)\n    2010 McGowan, K.B. Aerodynamic Modeling of Coarticulation for Unit Selection Synthesis.  Poster, 84th Annual Meeting of the Linguistic Society of America\n    2009 Beddor, P.S., McGowan, K.B., Boland, J., and Coetzee, A. The Perceptual Time Course of Coarticulatory Nasalization.  Poster, 158th Meeting of the Acoustical Society of America\n    2009 McGowan, K.B. Aerodynamic Modeling for Concatenative Speech Synthesis.  Poster, 158th Meeting of the Acoustical Society of America\n    2008 McGowan, K.B.  Pointwise Mutual Information and the Syllable Contact Law.  Mid-Continental Workshop on Phonology (MCWOP 14)\n    2008 Coetzee, A. and McGowan, K.B. Allophonic cues to syllabification.  CUNY Conference On The Syllable\n    \n    1994 Bailey, R.W. and Kevin B. McGowan The Michigan Academy — “English in Michigan: A Century of Scholarship” with Richard W. Bailey\n    \n    \n\n\n\n    Teaching\n    \n    University of Kentucky\n    Phonetics\n     Linguistics 500\n     Fall 2015—2017\n\n    Computational Linguistics\n     Linguistics 511\n     Fall 2017\n\n    Language in U.S. Society\n     Linguistics 331\n     Spring 2016—2017\n\n    Phonology\n     Linguistics 515\n     Spring 2017\n\n    Introduction to R\n     Arts & Sciences 500\n     Spring 2016—2017\n\n    Sociophonetics\n     Linguistics 617\n     Spring 2016\n\n    Introduction to Linguistics\n     Linguistics 221\n     Fall 2015—2016\n\n    Linguistic Society of America\n        Doing Phonetics Research\n    with Melissa Baese-Berk, Melinda Fricke, & Natasha Warner\n        LSA Linguistic Institute\n        Summer 2017, Lexington, KY\n    Eyetracking for linguistic research\n     LSA Institute\n         Summer 2017, Lexington, KY\n    Praat Scripting\n     LSA Annual Meeting Pre-Workshops\n     January 2015, Portland, OR \n     LSA Institute\n     Summer 2013, Ann Arbor, MI \n\n    Rice University\n    Computational Linguistics\n     Linguistics 409 Rice University, Department of\n    Linguistics Spring 2013\n\n    Introduction to Phonology\n    Linguistics 311/511 & Anthropology 323/523Rice University, Department of LinguisticsSpring 2012 & Spring 2013\n\n    Advanced Phonology\n    Linguistics 427Rice University, Department of LinguisticsFall 2012\n\n    Hearing & Speech Perception, Graduate Seminar\n    Linguistics 555Rice University, Department of LinguisticsSpring 2012\n\n    Introduction to the Scientific Study of Language\n    Linguistics/Anthropology 200Rice University, Department of LinguisticsFall 2011\n\n    Introduction to Phonetics\n    Linguistics/Anthropology 301/501Rice University, Department of LinguisticsFall 2011 & 2012\n\n    University of Michigan\n    Language & the Human Mind\n    Linguistics 209/Pyschology 242Graduate Student Instructor for Professor Sam EpsteinUniversity of Michigan, Department of LinguisticsFall 2009\n\n    College Writing\n    English 125University of Michigan, Department of English Language & LiteratureFall 2008\n\n    Introduction to Language\n    Linguistics 111University of Michigan, Department of LinguisticsSpring 2008\n\n    Javascript Programming\n    Introductory JavaScript Programming WorkshopUniversity of Michigan, School of InformationWinter 2006, Winter 2007\n\n    UNIX, perl programming, SQL, and various computing skills workshops\n    University of Michigan, Information Technology Division1997 - 1999\n    \n\n\n\n                    Fellowships and Awards\n    \n        March 2016 Confucius Institute Faculty China Curriculum Development Grant\n    2010 - 2011 Horace H. Rackham Predoctoral Fellowship, Horace H. Rackham School of Graduate Studies, University of Michigan\n    2010 Humanities Candidacy Fellowship, University of Michigan\n    2008 National Science Foundation, Honorable Mention, Graduate Research Fellowship\n    2008 Pre-Candidate Research Grant, Rackham Graduate Student Research Grant, Rackham Graduate School, University of Michigan\n    2007 Linguistic Society of America, LSA Linguistic Institute Fellowship, LSA Summer Institute, Stanford University\n    2006 NAACL Summer School, North American Chapter of the Association for Computational Linguistics\n    2005 - 2006 Non-Traditional Fellowship, Rackham Graduate School, University of Michigan\n    \n\n\n\n    Student Research Supervision\n    \n    Penelope Howe Investigation of cues to fricative voicing in dialects of Malagasy.  Rice University.  Qualifying Paper in progress, chair\n    Hussein Hizazi Formant Analysis of Jordanian Arabic Vowels: Emphasis Effect on F2 in Jordanian Arabic.  Rice University.  Qualifying Paper in progress, chair\n    John Galindo A psychoacoustic study of the influence of L1 lexical tone and pitch perception.  Rice University.  Qualifying Paper in progress, chair\n    Ling Ma Native Mandarin perception of English word boundary consonant overlap.  Qualifying Paper in progress, chair\n    Ru-ping Ruby Tso Chinese Characters and Speech Recognition.  Rice University.  Qualifying Paper in progress, reader\n    Obi Nwabueze The use of eye-tracking in speech perception research.  Undergraduate Research\n    Sheri-Ann Peckham Investigation of age-related perceptual plasticity using Jamaican Creole.  Undergraduate Research\n    \n\n\n\n    Service\n    \n    2014 - Peer Review, Lingua\n    2014 - Association for Laboratory Phonology, Web Committee & @LabPhon Twitter maintainer\n    2012,2014 Conference abstract review, Speech Science and Technology 2012, Sydney, Australia\n    2012 - Peer Review, Journal of Language & Speech\n    2011 - Peer Review, Journal of the Acoustical Society of America\n    2009 Conference abstract review, Michigan Linguistics Society\n    2007 Conference abstract review, Experimental Approaches to Optimality Theory (ExpOT)\n    \n\n\n\n    Departmental Service & Participation\n    \n    2011 - 2013 Organizer, Statistics for Linguistics Reading Group, Rice University\n    2011 - 2013 Faculty Associate, Hanszen College, Rice University\n    2010 - 2011 Linguistics Executive Committee, University of Michigan\n    2007 - 2009 Co-chair of Michigan Linguistics Colloquium Committee \n    2006 - 2011 Active partipant and presenter at:\n        \n             Phonetics/Phonology Discussion Group (Phondi) \n             Computational Linguistics Lab Group \n             Psycholinguistics Lab Group \n        \n    \n\n\n\n                    Selected Software Projects\n    \n     CoSign, Open Source Web Single SignOn ( co-founder, former developer ). \n     ChomskyBot, Pseudo-random Sentence Generator and Gentle Mockery Engine. \n     ldapweb, Web-based LDAP Directory Client. \n     kpasswd.cgi, Web-based Kerberos password changer \n    \n \n\n\n    Technologies\n    \n     Eyelink II head-mounted eye tracker,EG2-PCX Electroglottograph, EVA 2 pneumotachograph, Zonare Ultrasound, Praat, Wavesurfer, OpenSesame, sox, etc.\n     Python, Perl, C, Bourne shell, JavaScript, awk, tcl, SQL, PL/SQL\n     RDBMS systems (Oracle, MySQL, PostgreSQL), UNIX administration & programming (Linux, OpenBSD, MacOS, Solaris, etc.), HTML, XML, CSS, LaTeX\n    \n \n\n\n    Professional affiliations\n    \n    2010 -  Association for Laboratory Phonology\n    2009 -  Acoustical Society of America\n    2006 -  Linguistic Society of America\n    \n\n    Download PDF"
  },
  {
    "objectID": "teaching/courses-lsa/lsa2017-EyeTracking.html",
    "href": "teaching/courses-lsa/lsa2017-EyeTracking.html",
    "title": "Eye-Tracking for Linguistics Research",
    "section": "",
    "text": "This course is an introduction to eye tracking as a tool for understanding how language works. The instructor’s background is in phonetics and speech perception so there will necessarily be an emphasis on visual-world eye-tracking research in spoken word recognition. However, attention (pun intended) will also be given to sentence processing and reading research using eye tracking. Students completing this course will have the background needed to read, understand, and evaluate eye tracking literature. You will also have the foundation needed to design, implement, and analyze your own original eye tracking research. We will read and discuss foundational literature applying eye tracking to linguistic questions as well as a range of more recent papers covering both methodological and theoretical concerns. Finally, we will implement, collect data for, and analyze data for a visual world eye tracking experiment. The class will be most accessible if you have taken an introduction to psycholinguistics, had some exposure to phonetics, and have at least some familiarity using R for statistical data analysis (e.g. having completed the equivalent of this tutorial: http://www.cyclismo.org/tutorial/R/ ).\nAllopenna, P. D., Magnuson, J. S., & Tanenhaus, M. K. (1998). Tracking the time course of spoken word recognition using eye movements: Evidence for continuous mapping models. Journal of Memory and Language, 38, 419-439.\nAltmann, G. T. (2011). Language can mediate eye movement control within 100 milliseconds, regardless of whether there is anything to move the eyes to. Acta Psychologica, 137, 190- 200.\nAltmann, G. T. M. (2011). The mediation of eye movements by spoken language. In S. P. Liversedge, I. D. Gilchrist, & S. Everling (Eds.), The Oxford Handbook of Eye Movements (pp. 979-1003). Oxford: Oxford University Press.\nBarr, D.J. (2008). Analyzing “visual world” eyetracking data using multilevel logistic regression. Journal of Memory and Language. 59(4), 457-474\nBarr, D. J., Gann, T. M., & Pierce, R. S. (2011). Anticipatory baseline effects and information integration in visual world studies. Acta Psychologica, 137, 201-207.\nBeddor, P.S., McGowan, K.B., Boland, J.E., Coetzee, A.W., & Brasher, A. (2013). The Perceptual Time Course of Coarticulation. Journal of the Acoustical Society of America, 133, 2350-2366.\nDegen, J. & Tanenhaus, M.K. (2016). Availability of Alternatives and the Processing of Scalar Implicatures: A Visual World Eye-Tracking Study. Cognitive Science, Jan, 40(1), 172-201.\nDahan, D., Drucker, S.J., & Scarborough, R.A. (2008). Talker adaptation in speech perception: Adjusting the signal or the representations? Cognition, 108(3), 710-718.\nDahan, D., & Gaskell, G. M. (2007). The temporal dynamics of ambiguity resolution: Evidence from spoken-word recognition. Journal of Memory and Language, 57, 483-501.\nDahan, D., Magnuson, J. S., & Tanenhaus, M. K. (2001). Time course of frequency effects in spoken-word recognition: evidence from eye movements. Cognitive Psychology, 42, 317-367.\nDahan, D., Magnuson, J. S., Tanenhaus, M. K., & Hogan, E. M. (2001). Subcategorical mismatches and the time course of lexical access: Evidence for lexical competition. Language and Cognitive Processes, 16, 507-534.\nDahan, D., & Tanenhaus, M. (2005). Looking at the rope when looking for the snake: Conceptually mediated eye movements during spoken-word recognition. Psychonomics Bulletin & Review, 12, 453-459.\nEberhard, K.M. (1995). Eye movements as a window into real-time spoken language comprehension in natural contexts. Journal of Psycholinguistic Research, 24(6), 409-436.\nHuettig, F., & Altmann, G. T. (2011). Looking at anything that is green when hearing “frog”: how object surface colour and stored object colour knowledge influence language-mediated overt attention. Quarterly Journal of Experimental Psychology, 64, 122-145.\nHuettig, F., & McQueen, J. M. (2007). The tug of war between phonological, semantic and shape information in language-mediated visual search. Journal of Memory and Language, 57, 460-482.\nHuettig, F., Rommers, J., & Meyer, A. S. (2011). Using the visual world paradigm to study language processing: A review and critical evaluation. Acta Psychologica, 137, 151-171. *Magnuson, J. S., Dixon, J. A., Tanenhaus, M. K., & Aslin, R. N. (2007). The dynamics of lexical competition during spoken word recognition. Cognitive Science, 31, 133-156.\nMcMurray, B., Clayards, M. A., Tanenhaus, M. K., & Aslin, R. N. (2008). Tracking the time course of phonetic cue integration during spoken word recognition. Psychonomics Bulletin Review, 15, 1064-1071.\nMcMurray, B., Tanenhaus, M. K., & Aslin, R. N. (2009). Within-category VOT affects recovery from “lexical” garden paths: Evidence against phoneme-level inhibition. Journal of Memory and Language, 60, 65-91.\nMcQueen, J. M., & Viebahn, M. C. (2007). Tracking recognition of spoken words by tracking looks to printed words. Quarterly Journal of Experimental Psychology, 60, 661-671.\nMirman, D., Dixon, J. A., & Magnuson, J. S. (2008). Statistical and computational models of the visual world paradigm: Growth curves and individual differences. Journal of Memory and Language, 59, 475-494.\nRayner, K. (1998). Eye movements in reading and information processing: 20 years of research. Psychological bulletin, 124(3), 372. Chicago\nReinisch, E., Jesse, A., & McQueen, J. M. (2010). Early use of phonetic information in spoken word recognition: Lexical stress drives eye movements immediately. Quarterly Journal of Experimental Psychology, 63, 772-783.\nSalverda, A. P., Brown, M., & Tanenhaus, M. K. (2011). A goal-based perspective on eye movements in visual world studies. Acta Psycholica, 137, 172-180.\nSalverda, A. P., Dahan, D., Tanenhaus, M. K., Crosswhite, K., Masharov, M., & McDonough, J. (2007). Effects of prosodically modulated sub-phonetic variation on lexical competition. Cognition, 105, 466-476.\nShatzman, K. B., & McQueen, J. M. (2006). Segment duration as a cue to word boundaries in spoken-word recognition. Perception & Psychophysics, 68, 1-16.\nShen, J., Deutsch, D., & Rayner, K. (2013). On-line perception of Mandarin Tones 2 and 3: evidence from eye movements. Journal of the Acoustical Society of America, 133, 3016- 3029.\nTanenhaus, M. K. (2007). Spoken language comprehension: Insights from eye movements. Chicago\nTanenhaus, M. K. (2007). Eye movements and spoken language processing. Eye movements: A window on mind and brain, 309-26. Chicago"
  },
  {
    "objectID": "teaching/courses-lsa/lsa2017-EyeTracking.html#description",
    "href": "teaching/courses-lsa/lsa2017-EyeTracking.html#description",
    "title": "Eye-Tracking for Linguistics Research",
    "section": "",
    "text": "This course is an introduction to eye tracking as a tool for understanding how language works. The instructor’s background is in phonetics and speech perception so there will necessarily be an emphasis on visual-world eye-tracking research in spoken word recognition. However, attention (pun intended) will also be given to sentence processing and reading research using eye tracking. Students completing this course will have the background needed to read, understand, and evaluate eye tracking literature. You will also have the foundation needed to design, implement, and analyze your own original eye tracking research. We will read and discuss foundational literature applying eye tracking to linguistic questions as well as a range of more recent papers covering both methodological and theoretical concerns. Finally, we will implement, collect data for, and analyze data for a visual world eye tracking experiment. The class will be most accessible if you have taken an introduction to psycholinguistics, had some exposure to phonetics, and have at least some familiarity using R for statistical data analysis (e.g. having completed the equivalent of this tutorial: http://www.cyclismo.org/tutorial/R/ ).\nAllopenna, P. D., Magnuson, J. S., & Tanenhaus, M. K. (1998). Tracking the time course of spoken word recognition using eye movements: Evidence for continuous mapping models. Journal of Memory and Language, 38, 419-439.\nAltmann, G. T. (2011). Language can mediate eye movement control within 100 milliseconds, regardless of whether there is anything to move the eyes to. Acta Psychologica, 137, 190- 200.\nAltmann, G. T. M. (2011). The mediation of eye movements by spoken language. In S. P. Liversedge, I. D. Gilchrist, & S. Everling (Eds.), The Oxford Handbook of Eye Movements (pp. 979-1003). Oxford: Oxford University Press.\nBarr, D.J. (2008). Analyzing “visual world” eyetracking data using multilevel logistic regression. Journal of Memory and Language. 59(4), 457-474\nBarr, D. J., Gann, T. M., & Pierce, R. S. (2011). Anticipatory baseline effects and information integration in visual world studies. Acta Psychologica, 137, 201-207.\nBeddor, P.S., McGowan, K.B., Boland, J.E., Coetzee, A.W., & Brasher, A. (2013). The Perceptual Time Course of Coarticulation. Journal of the Acoustical Society of America, 133, 2350-2366.\nDegen, J. & Tanenhaus, M.K. (2016). Availability of Alternatives and the Processing of Scalar Implicatures: A Visual World Eye-Tracking Study. Cognitive Science, Jan, 40(1), 172-201.\nDahan, D., Drucker, S.J., & Scarborough, R.A. (2008). Talker adaptation in speech perception: Adjusting the signal or the representations? Cognition, 108(3), 710-718.\nDahan, D., & Gaskell, G. M. (2007). The temporal dynamics of ambiguity resolution: Evidence from spoken-word recognition. Journal of Memory and Language, 57, 483-501.\nDahan, D., Magnuson, J. S., & Tanenhaus, M. K. (2001). Time course of frequency effects in spoken-word recognition: evidence from eye movements. Cognitive Psychology, 42, 317-367.\nDahan, D., Magnuson, J. S., Tanenhaus, M. K., & Hogan, E. M. (2001). Subcategorical mismatches and the time course of lexical access: Evidence for lexical competition. Language and Cognitive Processes, 16, 507-534.\nDahan, D., & Tanenhaus, M. (2005). Looking at the rope when looking for the snake: Conceptually mediated eye movements during spoken-word recognition. Psychonomics Bulletin & Review, 12, 453-459.\nEberhard, K.M. (1995). Eye movements as a window into real-time spoken language comprehension in natural contexts. Journal of Psycholinguistic Research, 24(6), 409-436.\nHuettig, F., & Altmann, G. T. (2011). Looking at anything that is green when hearing “frog”: how object surface colour and stored object colour knowledge influence language-mediated overt attention. Quarterly Journal of Experimental Psychology, 64, 122-145.\nHuettig, F., & McQueen, J. M. (2007). The tug of war between phonological, semantic and shape information in language-mediated visual search. Journal of Memory and Language, 57, 460-482.\nHuettig, F., Rommers, J., & Meyer, A. S. (2011). Using the visual world paradigm to study language processing: A review and critical evaluation. Acta Psychologica, 137, 151-171. *Magnuson, J. S., Dixon, J. A., Tanenhaus, M. K., & Aslin, R. N. (2007). The dynamics of lexical competition during spoken word recognition. Cognitive Science, 31, 133-156.\nMcMurray, B., Clayards, M. A., Tanenhaus, M. K., & Aslin, R. N. (2008). Tracking the time course of phonetic cue integration during spoken word recognition. Psychonomics Bulletin Review, 15, 1064-1071.\nMcMurray, B., Tanenhaus, M. K., & Aslin, R. N. (2009). Within-category VOT affects recovery from “lexical” garden paths: Evidence against phoneme-level inhibition. Journal of Memory and Language, 60, 65-91.\nMcQueen, J. M., & Viebahn, M. C. (2007). Tracking recognition of spoken words by tracking looks to printed words. Quarterly Journal of Experimental Psychology, 60, 661-671.\nMirman, D., Dixon, J. A., & Magnuson, J. S. (2008). Statistical and computational models of the visual world paradigm: Growth curves and individual differences. Journal of Memory and Language, 59, 475-494.\nRayner, K. (1998). Eye movements in reading and information processing: 20 years of research. Psychological bulletin, 124(3), 372. Chicago\nReinisch, E., Jesse, A., & McQueen, J. M. (2010). Early use of phonetic information in spoken word recognition: Lexical stress drives eye movements immediately. Quarterly Journal of Experimental Psychology, 63, 772-783.\nSalverda, A. P., Brown, M., & Tanenhaus, M. K. (2011). A goal-based perspective on eye movements in visual world studies. Acta Psycholica, 137, 172-180.\nSalverda, A. P., Dahan, D., Tanenhaus, M. K., Crosswhite, K., Masharov, M., & McDonough, J. (2007). Effects of prosodically modulated sub-phonetic variation on lexical competition. Cognition, 105, 466-476.\nShatzman, K. B., & McQueen, J. M. (2006). Segment duration as a cue to word boundaries in spoken-word recognition. Perception & Psychophysics, 68, 1-16.\nShen, J., Deutsch, D., & Rayner, K. (2013). On-line perception of Mandarin Tones 2 and 3: evidence from eye movements. Journal of the Acoustical Society of America, 133, 3016- 3029.\nTanenhaus, M. K. (2007). Spoken language comprehension: Insights from eye movements. Chicago\nTanenhaus, M. K. (2007). Eye movements and spoken language processing. Eye movements: A window on mind and brain, 309-26. Chicago"
  },
  {
    "objectID": "teaching/courses-lsa/lsa2017-DoingPhoneticsResearch.html",
    "href": "teaching/courses-lsa/lsa2017-DoingPhoneticsResearch.html",
    "title": "Doing Phonetics Research",
    "section": "",
    "text": "Despite it’s framing as an introduction to phonetics, this course is not a foundational course for all or any particular area of phonetics, but rather is aimed at having us work through the process of forming an idea, and then working through all of the assumptions and issues which have to get dealt with. Throughout the course, we will llay out some basic and general, foundational concepts, and then have you start supplying the kernels of ideas. Then, each day we will work through crucial issues, such as assumptions about speakers and sampling, production and perception, measuring aspects of behavior and the idea of indices, and finding and interpreting background information on the topic. If all goes well, each student should have the basics of a research project sketched out."
  },
  {
    "objectID": "teaching/index.html#mentorship-advising",
    "href": "teaching/index.html#mentorship-advising",
    "title": "teaching",
    "section": "Mentorship & Advising",
    "text": "Mentorship & Advising\n\nEmily Remirez, co-chair with Keith Johnson\nNour Kayali\nAidah Aljuran\nJarred Brewster\nKyler Laycock\nOllie Combs\nKelly Wright\nKaitlyn Lee"
  },
  {
    "objectID": "teaching/index.html#external-examiner",
    "href": "teaching/index.html#external-examiner",
    "title": "teaching",
    "section": "External Examiner",
    "text": "External Examiner\n\nAndy M Gibson\nKsenia Gnevsheva"
  },
  {
    "objectID": "teaching/index.html#selected-courses-at-the-university-of-kentucky",
    "href": "teaching/index.html#selected-courses-at-the-university-of-kentucky",
    "title": "teaching",
    "section": "Selected Courses at the University of Kentucky",
    "text": "Selected Courses at the University of Kentucky\n\n\n\n\n\n\n\n\n\n\nLin 300 - Speech Sounds\n\n\n\n\n\n\nteaching phonetics phonology\n\n\n\n\n\n\n\n\n\nApr 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLin 500 - Introduction to Phonetics\n\n\n\n\n\n\nteaching phonetics\n\n\n\n\n\n\n\n\n\nApr 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLin 515 - Phonology\n\n\n\n\n\n\nteaching phonology\n\n\n\n\n\n\n\n\n\nApr 15, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#courses-at-linguistic-institutes",
    "href": "teaching/index.html#courses-at-linguistic-institutes",
    "title": "teaching",
    "section": "Courses at Linguistic Institutes",
    "text": "Courses at Linguistic Institutes\n\n\n\n\n\n\n\n\n\n\nSpeech Perception\n\n\nco-taught with Patrice S. Beddor\n\n\nteaching \"speech perception\"\n\n\n\n\n\n\n\n\nJul 15, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nDoing Phonetics Research\n\n\nco-taught with Melissa Baese-Berk and Natasha Warner\n\n\n\nteaching phonetics\n\n\n\n\n\n\n\n\n\nJul 15, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nEye-Tracking for Linguistics Research\n\n\n\n\n\nteaching \"speech perception\" eye-tracking\n\n\n\n\n\n\n\n\nJul 15, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nPraat Scripting\n\n\n\n\n\n\nteaching Praat methods\n\n\n\n\n\n\n\n\n\nJul 15, 2015\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/courses-uk/lin300.html",
    "href": "teaching/courses-uk/lin300.html",
    "title": "Lin 300 - Speech Sounds",
    "section": "",
    "text": "course information goes here."
  },
  {
    "objectID": "goat/index.html",
    "href": "goat/index.html",
    "title": "GOAT!",
    "section": "",
    "text": "GOAT"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "research",
    "section": "",
    "text": "“The thing about speech is, it varies.” (Remirez, in press). But for a number of historical and disciplinary reasons (see McGowan, in press), it feels normal for people in my field to talk about how listeners are “confronted with” tremendous variation in speech or to think about and write about variation as a big problem that listeners have to solve. I think this is a real shame.\n\n\n\nHi! My research takes the position that listeners are sensitive to patterns of covariation in speech and that knowing (more on ‘knowing’ later) these subtle patterns is part of what it means to truly know a language.\n\n\n\n\nWhy? Because speech is social. When we speak, we communicate not only our ideas, but also numerous social cues about who we are, where we come from, what we think you already know, who we think is listening, and many other details…\n\n\n\nWhich is great because if people sound the way we expect them to, we’re able to understand them better (McGowan, 2015; 2012 LSA presentation)\n\n\n  \n\n\n\nThis is true even when our expectations are based on stereotypes rather than authentic experience (McGowan, 2017)!\n\n\n\n\n\nIn Sumner, Kim, King, and McGowan (2014) we propose a model (below) of how the linguistic and the social aspects of speech interact to support perception.\n\n\n\n\nIn this model, we propose that listeners process both phonetically cued social information and phonetically cued linguistic information prior to word recognition and that these dual routes can interact to guide and facilitate perception.\n\n\n\nFor example, when Anna Babel and I told Bolivian listeners to expect Quechua-accented Spanish, they perceived vowels completely differently than if they were told to anticipate a Spanish accent (McGowan and Babel, 2020)\n\n\nThis suggests that experienced listeners have detailed knowledge of phonetically-cued social information —so expecting the variation consistent with a particular social category can change the way we perceive vowels. Social information isn’t noise to be thrown away, it is an essential part of the signal.\n\n\n\n\nSo here’s that word know again! Knowledge is not the same thing as awareness. We told listeners they’d changed speakers (from Quechua to Spanish or vice versa) and while they gave every indication of believing the switch their low level perceptual behavior stayed the same!\n\n\n\n\n\nSo does all this knowledge and sensitivity only apply to social variation?\n\n\n\nFirst, some quick background on how sounds like [p], [t], and [k] differ from sounds like [b], [d], and [g] at the beginning of English words like pit and bit. What word is this native American English speaker saying? Does it sound like ‘pit’ or ‘bit’ to you?     Your browser does not support the audio element. \n\n\n\n The image to the left is a spectrogram (frequency analysis over time) of the word pit. Hear the puff of air at the beginning? It is highlighted in blue.    Your browser does not support the audio element.  \n pit and bit both start with the lips completely closed. One of the main differences between them is the duration of the puff of air, this duration is called VOT (voice onset time).    Your browser does not support the audio element. \n \n\n[pʰɪt]\n\n\n[bɪt]\n\n\n\nAt least in American English, that puff of air is so important that if we cut it out of pit (this was that first sound you played! ‘pit’ with the puff removed) it results in a word that sounds to English-knowing listeners a lot like bit —though probably with maybe a funny [b], and that funniness is every bit as interesting and important as the change from [p] to [b]!\n\n\n\nAnother covarying feature is the way vowels before nasal consonants in English tend to be nasalized. Listeners can use this as soon as it becomes available, not only a large distinction like bend/bed…   Your browser does not support the audio element.    Your browser does not support the audio element. \n\n\nbut also a much more subtle distinction like the difference in nasalization between these two sound files. Can you hear a difference?\n\n\n\n This first recording has late nasalization starting 100 miliseconds after the [b].   Your browser does not support the audio element. \n This second recording has early nasalization starting 33 miliseconds after the [b].   Your browser does not support the audio element. \n\n\n\n\nIn an eye tracking task we found that listeners can use nasalization as soon as it is present. Looks to the heavily-nasalized word were, on average, 60 ms faster —the same average difference between early and late nasalization in the recordings (Beddor, McGowan, Boland, Coetzee, and Brasher, 2013).\n\n\n\n\nWhether the information is social, contextual, articulatory, or idiosyncratic, we humans have an astonishing ability to attend to it, remember it, and activate it during perception. This ability, my research suggests, is not irrelevant to linguistic competence or even peripheral to it, it is fundamentally and centrally part of what it means to know and speak a human language.\n\n\n Thank you for reading! Ask me questions? And please enjoy this pigeon engraving as a free gift. And many, many thanks to my friend M.C. Nee for turning me into this cartoon.\n\n\n\nDownload BibTex"
  },
  {
    "objectID": "stuff/posts/2014-07-11-python-preproccesing-script.html",
    "href": "stuff/posts/2014-07-11-python-preproccesing-script.html",
    "title": "Preprocess R data with python",
    "section": "",
    "text": "This script can be easily modified to code, recode, or modify csv files prior to loading in R.\n\n#  transform a raw data file into a happy proto data frame\n#\n\nimport csv\n\nf = open( 'transposed.csv' )\nhnps = csv.reader( f, delimiter = ',' )\nheader = hnps.next()\n\nverbs = [\n     'mentioned',\n     'indicated',\n     'proposed',\n     'suggested',\n     'recommended',\n     'confessed',\n     'stated',\n     'announced',\n     'muttered',\n     'explained',\n     ]\n\nout_name = 'reshaped.csv'\nout = open( out_name, 'w' )\ndata = csv.writer( out, delimiter = ',' )\ndata.writerow( [ 'subject', 'score', 'shift', 'length', 'verb' ])\n\nfor line in hnps:\n    shift_length = line[header.index('V1')]\n\n    try:\n    if shift_length[0] == \"N\":\n        shift = 'N'\n        length = shift_length[1:]\n    elif shift_length[0] == \"S\":\n        shift = 'S'\n        length = shift_length[1:]\n    except IndexError:\n    continue\n\n    sentence = line[header.index('ResponseID')]\n    for v in verbs:\n    # we're in trouble if one of the sentences contains more than one verb\n    # fortunately, they don't.\n    if v in sentence:\n        verb = v\n        break\n\n    for s in range( 1, 193 ):\n    try:\n        line[header.index( 'ID.' + str(s))]\n    except IndexError:\n        # that cell is empty for this subject\n        continue\n    else:\n        # if this subject responded to this item add data point to file\n        score = line[header.index( 'ID.' + str(s))]\n        if score.isdigit():\n        data.writerow( [ s, score, shift, length, verb ])\n\nf.close()\nout.close()"
  },
  {
    "objectID": "stuff/posts/2008-12-21-2^15.html",
    "href": "stuff/posts/2008-12-21-2^15.html",
    "title": "2^15",
    "section": "",
    "text": "In case anyone is wondering, the maximum number of files that MacOS X (v10.5.6) will allow you to drag and drop at once is 32,768.\nSo drag and drop responsibly this holiday season."
  },
  {
    "objectID": "stuff/posts/2013-01-20-repeatedFFT.praat.html",
    "href": "stuff/posts/2013-01-20-repeatedFFT.praat.html",
    "title": "repeatedFFT.praat",
    "section": "",
    "text": "This Praat editor script will draw a series of 6 FFT spectra starting from a cursor in the edit window. Use Add to dynamic menu… from the File menu to add to, say, the Spectrum menu. Useful for nasalization, diphthongs, and a thousand other purposes."
  },
  {
    "objectID": "stuff/posts/2015-03-24-create-publishable-spectrograms.html",
    "href": "stuff/posts/2015-03-24-create-publishable-spectrograms.html",
    "title": "Generate publishable spectrograms with Praat",
    "section": "",
    "text": "Don’t take a screen shot of that spectrogram! Use this script to create a publication-quality spectrogram from a selected wav file. Suitable for adding as an object window button in Praat. Makes a great gift!"
  },
  {
    "objectID": "stuff/posts/2008-08-08-vegetarian-french-onion.html",
    "href": "stuff/posts/2008-08-08-vegetarian-french-onion.html",
    "title": "Vegetarian french onion soup",
    "section": "",
    "text": "3 T olive oil\n4 vidalia onions sliced thin\n1 t kosher salt\n1 T dijon mustard\n1/4 t thyme\n6 cups vegetable stock\n2 T low sodium soy sauce\n1.5 t sherry vinegar\nfresh pepper\ncroutons (homemade, of course)\ngrated swiss and parmesan cheese\n\nBring olive oil in dutch oven to medium heat. Add onions and salt, sweat for 10 minutes, stirring occasionally.\nAdd mustard and thyme, stir well, cover.\nSimmer over very low heat for 35 minutes.\nAdd stock, soy sauce, vinegar, and pepper. Simmer for another 10 minutes.\nLadle the soup into bowls and top with croutons and cheese. If your bowls are oven-proof, place the dishes under the broiler for a minute to melt and brown the cheese."
  },
  {
    "objectID": "stuff/posts/2009-03-23-align.html",
    "href": "stuff/posts/2009-03-23-align.html",
    "title": "Forced-alignment and segmentation of airflow data",
    "section": "",
    "text": "An airflow system like the SQLab EVA2 used in our lab creates separate wav-like files for audio, oral airflow, and nasal airflow. Usually we use a program like wavesurfer or Praat to view these files and extract our measurements.\nBut what if you want to extract measurements for all of the segments in a reasonably-sized corpus of continuous speech? This can, of course, be done by hand but the process is tedious, labor-intensive, and inescapably subjective. This tutorial explains how to use Praat and HTK to segment audio recordings and use that segmentation to extract phone-level airflow data for an entire corpus. The segmentation is unlikely to be as good as a trained linguist might achieve, but it’s definitely faster and a very good starting point. The corpus I’m working with is a set of recordings of the TIMIT prompts I made for my doctoral candidacy qualifying project: Modeling Airflow for Concatenative Speech Synthesis.\nSolution\n\nConvert the EVA2 files to WAV format.\n\nSQLab distribute a piece of Windows-only, closed-source software called RIFF Edit for converting from their proprietary format to WAV format. Works great on my Mac using CrossOver which, I believe, is just a very nice version of wine.\n\nBreak the audio recordings up into separate files (one prompt/sentence per file) Of course there are many ways to do this. Usually one would record each prompt separately so that the audio files are naturally in separate files. The clumsy EVA2 system makes this arrangement difficult, though, so I used a TextGrid to mark each prompt on an interval tier. This can be done automatically using Praat’s “Annotate –&gt; To TextGrid (Silences)” command (some clean-up is usually necessary, but it saves a lot of work). Once the prompts are marked in the TextGrid I run this Praat script that I did not write.\n\n( As a quick aside, Kyle Gorman has written a terrific little Python class for manipulating TextGrids –does exactly what you’d want in just exactly the way you’d probably expect. )\n\nNow use the same TextGrid and script to extract prompts from the OAF (oral airflow) and NAF (nasal airflow) files.\n\nThis is the great strength of using something like Praat in the first place. Be sure to check a random sample of your alignments to make sure everything worked, but I’ve never had a problem.\n\nLabel all three files properly\n\nMy utterances are in a file called utts.data (identical to the file needed by Festival for voice creation) with the format:\n(audio_0003 \"This was easy for us.\")\n(audio_0004 \"Jane may earn more money by working hard.\")\n(audio_0005 \"She is thinner than I am.\")\n(audio_0006 \"Bright sunshine shimmers on the ocean.\")\n(audio_0007 \"Nothing is as offensive as innocence.\")\n(audio_0008 \"Why yell or worry over silly items?\")\n(audio_0009 \"Where were you while we were away?\")\n \nAnd then I run the shell script mkpf.sh to double check the audio and create the prompt files.\n\n\n#!/bin/bash\n\nexport UTTS=../utts.data\n\necho which file are we starting with [ 1 = first ]?\nread START;\n\nif [ \"$START\" == \"\" ]; then\n     START=1\nfi\n\nfor i in `ls a*.wav | sort -n -k1.2`; do\n    num=`printf \"%04d\" $START`\n\n    line=`grep \"audio_$num\" $UTTS`\n    prompt=`echo $line | awk -F\\\" '{print $2}'`\n    echo $line\n    afplay $i;\n    echo make $i $num [y] ?;\n\n    read ANSWER;\n\n    if [ \"$ANSWER\" == \"n\" ]; then\n        echo doing nothing\n    else\n        if [ \"$ANSWER\" == \"p\" ]; then\n            let \"START = START - 1\"\n            num=`printf \"%04d\" $START`\n\n            echo overwriting $i to $num ...\n        else\n            echo saving $i to $num ...\n        fi\n\n        cp $i audio_${num}.wav\n        cp oaf_$i oaf_${num}.wav\n        cp naf_$i naf_${num}.wav\n        echo $prompt &gt; ${num}.txt\n        rm $i\n\n        let \"START = START + 1\"\n    fi\ndone"
  },
  {
    "objectID": "stuff/posts/2009-02-28-css-interlinear-glosses.html",
    "href": "stuff/posts/2009-02-28-css-interlinear-glosses.html",
    "title": "Easy, readable css interlinear glosses",
    "section": "",
    "text": "CSS should make the creation and sharing of standard interlinear glosses/translations easy; so far it does not. In general, either the text entry or the output (or both!) is absolutely unacceptable. For example:\n&lt;h1&gt;John 3:16&lt;/h1&gt;\n&lt;div class=\"unit\"&gt;&lt;p class=\"gk\"&gt;οὕτως&lt;/p&gt;\n&lt;p class=\"en\"&gt;such&lt;/p&gt;&lt;/div&gt;\n&lt;div class=\"unit\"&gt;&lt;p class=\"gk\"&gt;γὰρ&lt;/p&gt;\n&lt;p class=\"en\"&gt;for&lt;/p&gt;&lt;/div&gt;\n&lt;div class=\"unit\"&gt;&lt;p class=\"gk\"&gt;ἠγάπησεν&lt;/p&gt;\n&lt;p class=\"en\"&gt;loved&lt;/p&gt;&lt;/div&gt;\n \nNotice that any sense of these translated words having some pairwise (let alone phrasal, clause, or sentence!) structure is completely shrouded in the cumbersome and obtrusive markup (this would also be a huge pain to type). The output is admittely really nice and allows one to do some cool things with javascript, but the text is essentially unusable in this form. If I wanted 4,000,000 tags per document I’d use XML/XSLT (p.s. yuck).\nThe goal, it seems,is something more like the entry one uses in gb4e.sty glosses in a LaTeX document:\n\\begin{exe} \n\\ex \n\\gll Wenn jemand in die W\\\"uste zieht ... \\\\ \nIf someone in the desert draws and lives ... \\\\ \n\\trans ‘if one retreats to the desert and ... ’ \n\\end{exe} \n \ngb4e/LaTeX handle making words that are glosses of one another line up in the display. In short, the goal is separation of content from display and that’s the entire bloody point of CSS, right?\nWhat I’d really like is text entry like this:\n&lt;div class=\"interlinear\"&gt;\n&lt;p class=\"source\"&gt;T&aacute; ceol agam. &lt;/p&gt;\n&lt;p class=\"gloss\"&gt;to be-PRS music-M.PL at-1SG&lt;/p&gt;\n&lt;p class=\"target\"&gt;I am musical&lt;/p&gt;\n&lt;/div&gt;\n \nIn addition to the problem that CSS is basically incapable of doing any kind of interesting text alignment, this example introduces the complication of aligning multiple words to a single word. What I have so far is text entry using the CSS inline-table attribute like this:\n&lt;div class=\"interlinear\"&gt;\n&lt;p class=\"gloss\"&gt;\n    &lt;div class=\"gll\"&gt;T&aacute;&lt;br /&gt;to be-PRS&lt;/div&gt;\n    &lt;div class=\"gll\"&gt;ceol&lt;br /&gt;music-M.PL&lt;/div&gt;\n    &lt;div class=\"gll\"&gt;agam&lt;br /&gt;at-1SG&lt;/div&gt;\n&lt;/p&gt;\n&lt;p class=\"translation\"&gt;I am musical&lt;/p&gt;\n&lt;/div&gt;\n \nwhich, I think, still totally sucks. There’s less markup than the first version so it’s slightly better, but it’s still not very good. Two good things, though, are that the CSS is obvious, simple and standard (interlinear.css) and the output is nice:\n\n\n\nIrish\n\n\nTáto be-PRS\n\n\nceolmusic-M.PL\n\n\nagamat-1SG\n\n\n\nI am musical\n\n\n\nLatin\n\n\nIn  in\n\n\nnomine  name\n\n\nPatris  Father\n\n\net  and\n\n\nFilii  Son\n\n\net  and\n\n\nSpiritus  Spirit\n\n\nSancti  Holy\n\n\n\nIn the name of the Father, the Son and the Holy Spirit.\n\n\n\nClassical Japanese (Ariwara no Narihira courtesy E. Alpert)\n\n\ntsuki moon\n\n\nya Q\n\n\naranu is-NEG\n\n\nharu spring\n\n\nya Q\n\n\nmukashi long.ago\n\n\nno GEN\n\n\nharu spring\n\n\nnaranu COP-NEG\n\n\n\n“Isn’t this the moon? And isn’t spring the way it used to be?”\n\n\n\n \nI have a little javascript hack that will assemble these inline-table divs at run time from the first type of HTML. This allows me to write markup in the style I want but it’s gross and still doesn’t facilitate searching. I’ll update this page when I come up with something better (or you could e-mail me if you have a better idea)."
  },
  {
    "objectID": "stuff/posts/2009-03-06-tuna-safe-tuna-salad.html",
    "href": "stuff/posts/2009-03-06-tuna-safe-tuna-salad.html",
    "title": "Tuna safe tuna salad",
    "section": "",
    "text": "Dolphins are great and all, but have you ever seen a living, swimming tuna? They’re bloody amazing.\n\n3 cups cooked, drained chick-peas\nseveral spoonfuls of mayo-like substance\n1 stalk minced celery\n1 T nutritional yeast flakes\n2 green onions, chopped\n2 tsp soy sauce\n1 teaspoon kelp powder (for that authentic “from the sea” flavor)\nkosher salt & fresh pepper, to taste\n\nMash up the chick peas; they deserve it. Seriously, take out all of your aggression on these bastards. Throw everything else in the bowl. Stir. Eat. Delicious."
  },
  {
    "objectID": "stuff/posts/2010-07-15.markdown-my-dissertation-prospectus.html",
    "href": "stuff/posts/2010-07-15.markdown-my-dissertation-prospectus.html",
    "title": "in a nutshell",
    "section": "",
    "text": "in a nutshell. My prospectus defense is on June 24th. From there it’s about another year until I’m done and have to find a job.\n\n\n\nwordcloud"
  },
  {
    "objectID": "stuff/index.html",
    "href": "stuff/index.html",
    "title": "stuff",
    "section": "",
    "text": "Useless\n\n\n\npoetry\n\n\n\n(A poem that is not about computers)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpen Source OCR on MacOS\n\n\n\nmacos sucks\n\n\n\n\n\n\n\n\n\n\nApr 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur souls refracted through a mesh\n\n\n\npoetry\n\n\n\n\n\n\n\n\n\n\nSep 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA child demonstrates the McGurk effect\n\n\n\nphonetics speech perception mcgurk\n\n\n\n\n\n\n\n\n\n\nMar 26, 2015\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate publishable spectrograms with Praat\n\n\n\npraat publishing phonetics\n\n\n\nPublishers love this one wild trick!\n\n\n\n\n\n\nMar 24, 2015\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreprocess R data with python\n\n\n\npython data\n\n\n\n(for archival purposes only, probably don’t use this.)\n\n\n\n\n\n\nJul 11, 2014\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmatch_tg_times.praat\n\n\n\npraat textgrids\n\n\n\n\n\n\n\n\n\n\nJul 10, 2014\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew Order’s Blue Monday synthesized in Praat (sorta)\n\n\n\npraat silly\n\n\n\n\n\n\n\n\n\n\nJun 16, 2013\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrepeatedFFT.praat\n\n\n\npraat phonetics\n\n\n\n\n\n\n\n\n\n\nJan 20, 2013\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArbitrary sinewave diagrams in LaTeX\n\n\n\nlatex phonetics teaching\n\n\n\n\n\n\n\n\n\n\nOct 9, 2012\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nin a nutshell\n\n\n\ndissertation\n\n\n\nMy dissertation prospectus in a nutshell\n\n\n\n\n\n\nJul 15, 2010\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s 1000 words worth?\n\n\n\ninformation\n\n\n\n\n\n\n\n\n\n\nJul 14, 2010\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForced-alignment and segmentation of airflow data\n\n\n\npraat airflow htk\n\n\n\n\n\n\n\n\n\n\nMar 23, 2009\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTuna safe tuna salad\n\n\n\nrecipes vegetarian\n\n\n\n\n\n\n\n\n\n\nMar 6, 2009\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEasy, readable css interlinear glosses\n\n\n\ncss linguistics\n\n\n\n(surely there is a better way to do this now?)\n\n\n\n\n\n\nFeb 28, 2009\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2^15\n\n\n\nmacos\n\n\n\n\n\n\n\n\n\n\nDec 21, 2008\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVegetarian french onion soup\n\n\n\nrecipes vegetarian\n\n\n\n\n\n\n\n\n\n\nAug 8, 2008\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimmoderate improvement\n\n\n\nperl celex\n\n\n\n\n\n\n\n\n\n\nApr 8, 2007\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "stuff/posts/2010-08-13-useless.html",
    "href": "stuff/posts/2010-08-13-useless.html",
    "title": "Useless",
    "section": "",
    "text": "another process has my pipe so I can’t read and I can’t write now what am I for on this long, lonely night if I can’t read and I can’t write?\nwhy do all we children have to fight for time and attention of one named pipe? now I’m all blocked up no end of file in sight and I can’t read and I can’t write."
  },
  {
    "objectID": "stuff/posts/2013-06-16-new-order-blue-monday-in-praat.html",
    "href": "stuff/posts/2013-06-16-new-order-blue-monday-in-praat.html",
    "title": "New Order’s Blue Monday synthesized in Praat (sorta)",
    "section": "",
    "text": "Let’s say that you, like me, love New Order’s song Blue Monday. For some reason, let’s imagine that you have a computer and Praat but no access to your music collection, youtube, spotify, or, um, any other part of the internet but somehow still have access to this blog. Okay, no, that’s silly. Here is a Praat script that will sort of play part of this great song.\n# plays Blue Monday by New Order, sort of.\n# 16 July 2013, Kevin B. McGowan &lt;kbmcgowan@stanford.edu&gt;\n\nsong$ = \"349.23, 261.63, 293.66, 293.66, 392.00, 261.63, 293.66, 293.66\"\n\nfor reps from 1 to 2\n     @split( \",\", song$ )\n\n     for note from 1 to split.length\n         @playPairs( number( split.array$[note] ))\n     endfor\nendfor\n\nprocedure playPairs( .note )\n    # first generate one octave below our note\n    .octaveID = do( \"Create Sound as pure tone...\", \"note\", 1, 0, 0.05,\n            ...10000, .note / 2, 0.7, 0.01, 0.09 )\n\n    .noteID = do ( \"Create Sound as pure tone...\", \"note\", 1, 0, 0.05,\n            ...10000, .note, 0.7, 0.01, 0.09 )\n\n    for i from 1 to 2\n        # play each octave pair twice\n        selectObject( .octaveID )\n        do( \"Play\" )\n\n        selectObject( .noteID )\n        do( \"Play\" )\n    endfor\n\n    removeObject( .octaveID )\n    removeObject( .noteID )\nendproc\n\n# split proc written by Jose J. Atria (28 February 2012)\nprocedure split( .sep$, .str$ )\n    .length = 0\n    repeat\n        .strlen = length( .str$ )\n        .sep = index( .str$, .sep$ )\n        if .sep &gt; 0\n            .part$ = left$( .str$, .sep - 1 )\n            .str$ = mid$( .str$, .sep + 1, .strlen )\n        else\n            .part$ = .str$\n        endif\n\n        .length = .length+1\n        .array$[.length] = .part$\n    until .sep = 0\nendproc"
  },
  {
    "objectID": "stuff/posts/2015-03-26-mcgurk-effect.html",
    "href": "stuff/posts/2015-03-26-mcgurk-effect.html",
    "title": "A child demonstrates the McGurk effect",
    "section": "",
    "text": "A few years ago I made this video of my daughter demonstrating the McGurk Effect (McGurk and MacDonald 197). This effect is a demonstration of the importance of visual information during perception. Or, perhaps, it is best understood as a demonstration of the importance of listener expectations and how they can be established in real time by stunningly subtle cues.\nTry opening and closing your eyes while the girl is talking. What do you hear with your eyes open? What do you hear when they’re closed?\n\nThe audio you hear is “ba ba ba”, the video you see is of the same vocal tract producing “ga ga ga”, most commonly the percept listeners experience is of a somewhat odd “da da da”.\n@article{mcgurk1976hearing,\n  title={Hearing lips and seeing voices},\n  author={McGurk, Harry and MacDonald, John},\n  year={1976},\n  publisher={Nature Publishing Group}\n}"
  },
  {
    "objectID": "stuff/posts/2012-10-09-LaTeX-sinewaves.html",
    "href": "stuff/posts/2012-10-09-LaTeX-sinewaves.html",
    "title": "Arbitrary sinewave diagrams in LaTeX",
    "section": "",
    "text": "I needed to be able to generate a number of figures for a phonetics homework assignment showing an arbitrary number of component sine waves and the complex wave you get if you sum them.\nTikZ and gnuplot to the very enjoyable and useful rescue! The only tricky part is that Tikz wants to call gnuplot to plot the functions. I don’t let LaTeX execute arbitrary commands (for what are probably obvious reasons), but tikz generates intermediate &lt;filename&gt;.gnuplot files so I just had to add a line to execute these with gnuplot to the makefile for my homework assignment.\nExample one: and the code to create it.\n\n\\begin{tikzpicture}[domain = 0:16, samples = 1000]\n%\\draw[very thin, color = gray, step=1] (0,-2.75) grid(16,2.75);\n\n% zero crossing\n\\draw[dotted] (0,0) -- (16,0);\n\n% y axis and tick marks\n\\draw (0,3) -- (0,-3) node[below] {$amplitude$};\n\\draw (-.25,3) -- (.25,3) node at (-.5,3) {3};\n\\draw (-.25,2) -- (.25,2) node at (-.5,2) {2};\n\\draw (-.25,1) -- (.25,1) node at (-.5,1) {1};\n\\draw (-.25,0) -- (.25,0) node at (-.5,0) {0};\n\\draw (-.25,-1) -- (.25,-1) node at (-.5,-1) {-1};\n\\draw (-.25,-2) -- (.25,-2) node at (-.5,-2) {-2};\n\\draw (-.25,-3) -- (.25,-3) node at (-.5,-3) {-3};\n\n% give some sense of time...\n\\draw (15.2,3) -- (15.2,-3) node[below] {$.01s$};\n\n\\draw[very thick, dashed] plot[id=a] function{sin(2.75 * x)};\n\\draw[thick] plot[id=b] function{1.25 * sin(5 * x)};\n\\draw[very thick, color = blue] plot[id=c] function{sin(2.75 * x) + 1.25 * sin( 5 * x)};\n\\end{tikzpicture}\nExample two: and the code to create it.\n\n\\begin{tikzpicture}[domain = 0:16, samples = 2000]\n%\\draw[very thin, color = gray, step=1] (0,-2.75) grid(16,2.75);\n\n% zero crossing\n\\draw[dotted] (0,0) -- (16,0);\n\n\\draw (0,3) -- (0,-3) node[below] {$amplitude$};\n\\draw (-.25,3) -- (.25,3) node at (-.5,3) {3};\n\\draw (-.25,2) -- (.25,2) node at (-.5,2) {2};\n\\draw (-.25,1) -- (.25,1) node at (-.5,1) {1};\n\\draw (-.25,0) -- (.25,0) node at (-.5,0) {0};\n\\draw (-.25,-1) -- (.25,-1) node at (-.5,-1) {-1};\n\\draw (-.25,-2) -- (.25,-2) node at (-.5,-2) {-2};\n\\draw (-.25,-3) -- (.25,-3) node at (-.5,-3) {-3};\n\n\\draw (6.25,3) -- (6.25,-3) node[below] {$4ms$};\n    \n\\draw[very thick] plot[id=foo] function{sin(3*x)};\n\\draw[thick, dashed] plot[id=bar] function{2 * sin(x + 0.125)};\n\\draw[very thick, color = blue] plot[id=baz] function{sin(3*x) + 2 * sin(x + 0.125)};\n\\end{tikzpicture}"
  },
  {
    "objectID": "stuff/posts/2010-07-14-what's-1000-words-worth.html",
    "href": "stuff/posts/2010-07-14-what's-1000-words-worth.html",
    "title": "What’s 1000 words worth?",
    "section": "",
    "text": "Let’s randomly select 1,000 lines from the dictionary and appends the number of bytes in that sample to a file.\nfor i in {1..500}; do\n    awk 'BEGIN {srand()} {printf \"%05.0f %s \\n\",rand()*99999, $0; }' /usr/share/dict/words | sort -n |\\\\\n    head -1000 | sed 's/^[0-9]* //' | dd 2&gt;&1 | grep \"bytes transferred\" | awk '{print $1}' &gt;&gt;sizes.dat\ndone\n \nthen, in R:\n \n&gt; sizes &lt;- read.table(\"~/sizes.dat\", header=TRUE)\n&gt; mean(sizes)\n   bytes \n11581.83 \n&gt; sd(sizes)\n   bytes \n90.32316 \n&gt; qqnorm(sizes$bytes)\n&gt; plot(density(sizes$bytes))\n&gt; hist(sizes$bytes, col=rainbow(15, start=.4))\n&gt; mean(sizes$bytes) / 1024\n[1] 11.31038\n \n11.31k is not a very large picture. Each of the exploratory plots (quantile x normal, density, histogram) is larger! Even these pictures of me and my daughter and the fat giraffes are 13k, 13k, and 15k respectively:\n  \nStill, here are all the many, many google image hits for ‘entropy’ pictures that are 128 x 128 pixels. Many of these are are in the roughly 11k range.\nHappy Bastille Day."
  },
  {
    "objectID": "stuff/posts/2007-04-08-immoderate-improvement.html",
    "href": "stuff/posts/2007-04-08-immoderate-improvement.html",
    "title": "immoderate improvement",
    "section": "",
    "text": "I needed to calculate the number of consonant bigrams in English monomorphemic words from CELEX. The hash ‘bigrams’ contains the 30^2 possible consonant clusters given the DISC transcriptions, the transcription has already had stress and syllabification stripped from it. The first version of the code did it this way:\nforeach my $bg ( keys( %bigrams )) {\n    $bigrams{ $bg }++ while ( $PhonStrsDISC =~ /$bg/g );\n}\nthat loop looks for 900 unique strings, globally, in each of about 12,000 words and time looks like this:\nreal    0m58.995s\nuser    0m58.254s\nsys     0m0.221s\nthe second version does it this way:\nmy @phones = split( //, $PhonStrsDISC );\nfor ( my $i = 0; $i &lt; $#phones; $i++ ) {\n    my $bg = $phones[ $i ] . $phones [ $i + 1 ];\n    $bigrams{ $bg }++ if ( exists( $bigrams{ $bg } ));\n}\nI get exactly the same output. The time differences?!?!\nreal    0m0.730s\nuser    0m0.706s\nsys     0m0.026s\nmoral of the story: if you’re going to be an idiot, at least be a moderately efficient idiot. What I find ironic here is that the first pass is about as idiomatic as perl comes whiles the second is slightly less perlish (apart from the heavy reliance on split() and hashes, I mean). Normally I think of these well-worn perl idioms as trading accessibility to outsiders for speed."
  },
  {
    "objectID": "stuff/posts/2018-09-10-typishly.html",
    "href": "stuff/posts/2018-09-10-typishly.html",
    "title": "Our souls refracted through a mesh",
    "section": "",
    "text": "Typishly has published my latest poem: Our souls refracted through a mesh. I’m thrilled to have it published and by the beautiful job they did with the presentation."
  },
  {
    "objectID": "stuff/posts/2014-07-10-fix-textgrid-times.html",
    "href": "stuff/posts/2014-07-10-fix-textgrid-times.html",
    "title": "match_tg_times.praat",
    "section": "",
    "text": "This script fixes a weird problem when concatenating TextGrids and sound objects. Often a TextGrid and its associated sound object will differ by anywhere from fraction of a millisecond to a few milliseconds.\nThis is not a problem until you try to concatenate the TextGrids and still align them with their concatenated sound objects when suddenly intervals and points later in the chain will begin to be increasingly misaligned."
  },
  {
    "objectID": "stuff/posts/2020-04-15-ocr-macos.html",
    "href": "stuff/posts/2020-04-15-ocr-macos.html",
    "title": "Open Source OCR on MacOS",
    "section": "",
    "text": "MacOS Catalina (10.15) broke many things on my computer, but the one that has wasted the most of my time so far while I try to make old PDFs available to vision-impaired students is the fact that Apple broke Adobe Acrobat Pro and the nasty Creative Cloud license manager that goes with it. I lost most of a day trying to get all that closed source nastiness working again before deciding to solve the problem the way I always should have, with open source software. This little script uses ghostscript and tesseract to turn an image pdf into an OCRd version of that pdf."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "linguist, phonetician, nerd",
    "section": "",
    "text": "I am an Associate Professor and Director of Graduate Studies in the Department of Linguistics at the University of Kentucky where I also direct the UK Phonetics Lab."
  },
  {
    "objectID": "index.html#about-kevin-barry",
    "href": "index.html#about-kevin-barry",
    "title": "linguist, phonetician, nerd",
    "section": "About ‘Kevin Barry’",
    "text": "About ‘Kevin Barry’\nI’m named after a dance my mother choreographed for the Oireachtas Rince na Cruinne about a young fella who was somehow a medical student at a wisened 18 years of age and was executed by British occupying forces in 1920. The anglicization of the Irish name Caoimhín, ‘Kevin’ [’kʰɛ.vĩn] is, infamously, cognate with a German word meaning something like thug or, forgive me, chav."
  },
  {
    "objectID": "index.html#about-mcgowan",
    "href": "index.html#about-mcgowan",
    "title": "linguist, phonetician, nerd",
    "section": "About ‘McGowan’",
    "text": "About ‘McGowan’\nVariously anglicized as Mc Gowan, MacGowan, Gowan, or even Smythe or Smith, my surname is from the Irish Mac Gabhann [mə’gɑ.wĩn] which, Mac indicating a son of Gabhann meaning blacksmith or smith. Similar anglicization happens with Mac an Gabhann and unrelated, but similar, names in Scotland such that `McGowan’ is an incredibly common name."
  },
  {
    "objectID": "index.html#theres-a-lot-of-us.",
    "href": "index.html#theres-a-lot-of-us.",
    "title": "linguist, phonetician, nerd",
    "section": "There’s a lot of us.",
    "text": "There’s a lot of us.\n\nKevin J. McGowan, crow expert, is my favorite other Kevin McGowan, but don’t tell that to\nKevin B. McGowan there are so many Kevin McGowans in the world that there is even another Kevin B McGowan who works on nasal stuff!\nKevin McGowan, baseball player, stole our wikipedia entry simply by being way more famous than the rest of us.\nKevin McGowan, drummer & also possibly musician?\nKevin McGowan, molecular genomics\nKevin P. McGowan statistical programmer, alas, is no longer with us.\nThere are more, a lot more, but you get the idea."
  },
  {
    "objectID": "teaching/courses-uk/lin500.html",
    "href": "teaching/courses-uk/lin500.html",
    "title": "Lin 500 - Introduction to Phonetics",
    "section": "",
    "text": "course information goes here."
  },
  {
    "objectID": "teaching/courses-uk/lin515.html",
    "href": "teaching/courses-uk/lin515.html",
    "title": "Lin 515 - Phonology",
    "section": "",
    "text": "course information goes here."
  },
  {
    "objectID": "teaching/courses-lsa/lsa2019/index.html",
    "href": "teaching/courses-lsa/lsa2019/index.html",
    "title": "Speech Perception",
    "section": "",
    "text": "2019 Linguistic Institute: Speech Perception (221) Patrice (Pam) Speeter Beddor beddor@umich.edu Kevin B. McGowan kbmcgowan@uky.edu\nMonday & Thursday 1:05-2:30 pm\nExperimental speech perception, which spans a period of more than 70 years, investigates how listeners interpret the input acoustic signal as linguistic forms. From the discipline’s earliest years, researchers recognized that the acoustic signal is highly variable and that perceptual processing is more complex (and interesting!) than a simple one-to-one mapping between acoustic property and linguistic percept. Yet, despite this complexity, humans are highly accurate perceivers of the intended speech in typical conversational interactions.\nThis course will provide students with an overview of the dominant theories of speech perception and the theoretical issues that drive empirical studies, including the fundamental question of whether speech perception differs from other types of auditory processing. Readings, course discussions, and hands-on experience with classic speech perception experiments will guide students through the field’s evolution from an emphasis on psychoacoustics and the acoustic signal to an appreciation for the structured nature of variation and how it informs perception. We will discover together that, while listeners closely attend to the structured variation, individual listeners do so in ways that depend on their linguistic experiences, social expectations, processing style, and more. Listeners are active participants who recruit multiple cognitive resources in achieving malleable, dynamic perception.\n\nBackground: Students may find having some familiarity with acoustic phonetics helpful (although this background is not essential for this course). Excellent chapters include these:\n\no Raphael, L.J., Borden, G.J., & Harris, K.S. 2011. Speech Science Primer. Philadelphia, PA: Wolters Kluwer/Lippincott. 6th edition, chapters 5 & 6. o Reetz, H. & Jongman, A. 2008. Phonetics: Transcription, Production, Acoustics, and Perception. Malden, MA: Wiley-Blackwell. 1st edition, chapters 9 & 10. o Ladefoged, P. & Johnson, K. 2015. A Course in Phonetics. Boston, MA: Thomson. 6th edition, chapter 8. * Readings: There is no expectation that you will have time, during the busy weeks of the Institute, to read more than one paper per class meeting. For each class meeting, the syllabus marks the most strongly recommended reading in bold. (You might view other readings as background material that you may wish to consult after the Institute is over.) * Writing assignment: A roughly 500 word research prospectus for an original, theoretically motivated speech perception experiment or a two-page critical assessment, supported by experimental evidence from the literature, of a well-defined issue in speech perception. Students are encouraged to get together with Pam or Kevin to run their ideas past us before submitting their paper, which is due July 18 (11:59 pm)."
  },
  {
    "objectID": "teaching/courses-lsa/lsa2013-PraatScripting.html",
    "href": "teaching/courses-lsa/lsa2013-PraatScripting.html",
    "title": "Praat Scripting",
    "section": "",
    "text": "This course was an introduction to scripting in Praat in 2015. As of this update (summer 2024), I would recommend using Python (possibly with Praat) for any analysis that is not embedded within Praat’s user interface. The following materials are still relevant as an introduction to Praat scripting for someone who has never done it before and may be useful for anyone wanting to add custom menu commands to Praat."
  },
  {
    "objectID": "teaching/courses-lsa/lsa2019-SpeechPerception.html",
    "href": "teaching/courses-lsa/lsa2019-SpeechPerception.html",
    "title": "Speech Perception",
    "section": "",
    "text": "course information goes here."
  }
]